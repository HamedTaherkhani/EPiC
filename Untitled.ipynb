{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac3e7f5-4739-405d-8d0b-cec18cf595f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2020/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2020/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/gensim-4.0.1+computecanada-cp39-cp39-linux_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.11.3 in /project/6087623/hamedth/venv3.9/lib/python3.9/site-packages (from gensim) (1.25.2+computecanada)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /project/6087623/hamedth/venv3.9/lib/python3.9/site-packages (from gensim) (1.11.2+computecanada)\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/smart_open-6.4.0+computecanada-py3-none-any.whl (from gensim)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.0.1+computecanada smart-open-6.4.0+computecanada\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60815162-2e2c-47ef-b7c1-1d965b6dedc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6087623/hamedth/venv3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 84.9kB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 2.54MB/s]\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f18f1866-45b0-4132-af8e-750a454ed039)')' thrown while requesting HEAD https://huggingface.co/codellama/CodeLlama-7b-instruct-hf/resolve/main/special_tokens_map.json\n",
      "Downloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.75MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/hamedth/projects/def-hemmati-ac/hamedth/hugging_face'\n",
    "from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n",
    "import torch\n",
    "codeLLama_tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-instruct-hf\", device_map='auto')\n",
    "codeLLama_model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-instruct-hf\", device_map='auto')\n",
    "# codeLLama_model = codeLLama_model.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efcc257-100c-4fcc-adae-0ae287a954d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/hamedth/projects/def-hemmati-ac/hamedth/hugging_face'\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "MAGICODER_PROMPT = \"\"\"You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions.\n",
    "\n",
    "@@ Instruction\n",
    "{instruction}\n",
    "\n",
    "@@ Response\n",
    "\"\"\"\n",
    "\n",
    "instruction = \"def add(a,b):\"\n",
    "\n",
    "prompt = MAGICODER_PROMPT.format(instruction=instruction)\n",
    "magic_coder = pipeline(\n",
    "    model=\"ise-uiuc/Magicoder-S-DS-6.7B\",\n",
    "    task=\"text-generation\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "result = magic_coder(prompt, max_length=1024, num_return_sequences=1, temperature=0.0)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a0d02-fd03-4a37-8eff-0344d7609183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "human_eval=load_dataset(\"openai_humaneval\")\n",
    "\n",
    "final_test_cases = []\n",
    "for a_test in human_eval['test']:\n",
    "  method_name = re.findall('def .*\\(',a_test['prompt'])[0].replace('def ','').replace('(','')\n",
    "  test = a_test['test'] + '\\ncheck(' + method_name + ')'\n",
    "  final_test_cases.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e49b7-37f6-4e21-be3f-76e26ce3420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "code_eval_metric = load(\"code_eval\")\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "def ssplit(fil):\n",
    "  aas = fil.split('def')\n",
    "  temp = aas[0] + 'def' + aas[1]\n",
    "  return temp\n",
    "\n",
    "# def process_magic_coder_filling(filling):\n",
    "    \n",
    "def process_the_code_magic_coder(fillings):\n",
    "    processed_fillings = []\n",
    "    for index, a_fil in enumerate(fillings):\n",
    "        method_names = re.findall('def .*\\(',human_eval['test'][index]['prompt'])\n",
    "        number_of_methods = len(method_names)\n",
    "        try:\n",
    "            if index in (106, 119):\n",
    "                filling = a_fil\n",
    "                processed_fillings.append(filling)\n",
    "                continue\n",
    "            a_fil = a_fil.replace('print','#print')\n",
    "            methods = a_fil.split('def ')\n",
    "            if number_of_methods == 1:\n",
    "                filling = methods[0] + 'def ' + methods[1]\n",
    "            elif number_of_methods == 2:\n",
    "                filling = methods[0] + 'def ' + methods[1] + 'def ' + methods[2]\n",
    "            filling = filling.split('# Test cases')[0]\n",
    "        except Exception as e:\n",
    "            print(index)\n",
    "            filling = a_fil\n",
    "        processed_fillings.append(filling)\n",
    "    return processed_fillings\n",
    "\n",
    "def process_a_code_magic_coder(filling, index):\n",
    "    method_names = re.findall('def .*\\(',human_eval['test'][index]['prompt'])\n",
    "    number_of_methods = len(method_names)\n",
    "    try:\n",
    "        if index in (106, 119):\n",
    "            return filling\n",
    "        filling = filling.replace('print','#print')\n",
    "        methods = filling.split('def ')\n",
    "        if number_of_methods == 1:\n",
    "            filling = methods[0] + 'def ' + methods[1]\n",
    "        elif number_of_methods == 2:\n",
    "            filling = methods[0] + 'def ' + methods[1] + 'def ' + methods[2]\n",
    "        filling = filling.split('# Test cases')[0]\n",
    "    except Exception as e:\n",
    "        print(index)\n",
    "        return filling\n",
    "\n",
    "    return filling\n",
    "\n",
    "def evaluate_prompt(test_cases, prompt, solution=None, model_to_test=0, prompt_index=None):\n",
    "  if model_to_test == 0:\n",
    "      prompt = codeLLama_tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to('cuda:0')\n",
    "      generated_id = codeLLama_model.generate(prompt.replace('#SPECIAL_TOKEN',''), max_new_tokens=128)\n",
    "      filling = codeLLama_tokenizer.batch_decode(generated_id, skip_special_tokens = True)[0]\n",
    "      ## process\n",
    "      try:\n",
    "        aas = filling.split('def')\n",
    "        filling = aas[0] + 'def' + aas[1]\n",
    "      except Exception as e:\n",
    "        # print(prompt)\n",
    "        return 0,0\n",
    "  elif model_to_test == 1:\n",
    "      filling = magic_coder(prompt.replace('#SPECIAL_TOKEN',''), max_length=512, num_return_sequences=1, temperature=0.0)[0]['generated_text']\n",
    "      filling = process_a_code_magic_coder(filling, prompt_index)\n",
    "  ##\n",
    "  candidate = [filling]\n",
    "  candidates = [candidate]\n",
    "  pass_at_k, results = code_eval_metric.compute(references=[test_cases], predictions=candidates, k=[10])\n",
    "  if solution is not None:\n",
    "    # try:\n",
    "    #   temp = candidate[0].split(\"\\\"\"\"\")\n",
    "    #   temp[3] = ''\n",
    "    #   temp = ''.join(temp)\n",
    "    #   candidate = temp.split()\n",
    "    # except IndexError:\n",
    "    #   candidate = candidate[0].split()\n",
    "    # reference = solution.split()\n",
    "    # # print(solution)\n",
    "    # # print(temp)\n",
    "    # # print(solution)\n",
    "    # # print(temp)\n",
    "    # bleu_score = sentence_bleu(candidate, reference, weights = (1, 0, 0, 0))\n",
    "    bleu_score = 1 ## no use of this parameter \n",
    "    return pass_at_k['pass@1'], bleu_score\n",
    "  else:\n",
    "    return pass_at_k['pass@1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066a0ae1-2642-44aa-a27b-838e013ae4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6087623/hamedth/venv3.9/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7aca82-2e2e-4a87-894a-7317448a6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b2d2eb-44fd-4664-b062-50e1f14c46fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_criteria_met(evaluations):\n",
    "    if len(evaluations) < 4:\n",
    "        return False\n",
    "    if evaluations[-1][0] <= evaluations[-2][0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "evaluations = [(1,2), (2,3), (3,4),(4,5),(6,7),(5,6)]\n",
    "stop_criteria_met(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0dd29-3b1c-4861-9212-dd3f0f4bf9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bf12d-d461-49fb-846c-12e8a1ed93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "# base_prompts_re = []\n",
    "# for idx,base_prompts in enumerate(gpt_prompts):\n",
    "#   if idx not in codellama7_errors:\n",
    "#     base_prompts_re.append([human_eval['test'][idx]['prompt']])\n",
    "#   else:\n",
    "#     base_prompts_re.append(base_prompts)\n",
    "\n",
    "# base_prompts_re = all_generated_promts[]\n",
    "all_generated_promts = []\n",
    "# all_generated_promts = []\n",
    "evaluations = []\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "number_of_tests = 164\n",
    "iterations = 0\n",
    "\n",
    "model_to_test = 1 # 0 codeLama, 1 Magiccoder\n",
    "\n",
    "if model_to_test == 1:\n",
    "    base_prompts_re = base_prompts_re_codemagic.copy()\n",
    "\n",
    "passed_fillings = [0 for i in range(164)]\n",
    "while(not stop_criteria_met())\n",
    "  iteration += 1\n",
    "  all_generated_promts.append(base_prompts_re.copy())\n",
    "  ## evaluation\n",
    "  chosen_prompts = [rr[0] for rr in base_prompts_re[0:number_of_tests]] ##here\n",
    "  if iteration != 1000:\n",
    "      if model_to_test == 0:\n",
    "          fillings = []\n",
    "          for a_token in tqdm(chosen_prompts):\n",
    "            if not validate_prompt(a_token): ##this is because codeLLama_model has no max_new_tokens set and generates infinite output\n",
    "                filling = 'teeeeeeeeeeeeeeeeest'\n",
    "                fillings.append([filling])\n",
    "                continue\n",
    "            prompt = codeLLama_tokenizer(a_token, return_tensors=\"pt\")[\"input_ids\"]##.to('cuda:0')\n",
    "            generated_id = codeLLama_model.generate(prompt, max_new_tokens=128)\n",
    "            filling = codeLLama_tokenizer.batch_decode(generated_id, skip_special_tokens = True)[0]\n",
    "            try:\n",
    "                aas = filling.split('def')\n",
    "                filling = aas[0] + 'def' + aas[1]\n",
    "            except IndexError:\n",
    "                filling = 'teeeeeeeeeeeeeeeeest'\n",
    "            fillings.append([filling])\n",
    "      elif model_to_test == 1:\n",
    "          fillings = []\n",
    "          for a_token in tqdm(chosen_prompts):\n",
    "              filling = magic_coder(a_token.replace('#SPECIAL_TOKEN',''), max_length=800, num_return_sequences=1, temperature=0.0)[0]['generated_text']\n",
    "              fillings.append(filling)\n",
    "          fillings =  process_the_code_magic_coder(fillings)\n",
    "          fillings = [[fil] for fil in fillings]\n",
    "      errorrrs = []\n",
    "      pass_at_k, results = code_eval_metric.compute(references=final_test_cases[0:number_of_tests], predictions=fillings, k=[1]) ##here \n",
    "      print(f'iteration {iteration} pass@1 -------------------------------------\\n')\n",
    "      print(pass_at_k)\n",
    "      evaluations.append((pass_at_k, results))\n",
    "      \n",
    "      # for key,item in results[1].items():\n",
    "      #     if item[0][1]['passed']:\n",
    "      #         passed_fillings[item[0][1]['task_id']] = fillings[item[0][1]['task_id']][0]\n",
    "  ## evaluations\n",
    "\n",
    "  for idx, a_prompt_set in tqdm(enumerate(base_prompts_re[0:number_of_tests])): ##here\n",
    "    print(idx)\n",
    "    passed = False\n",
    "    if len(a_prompt_set) == 1:\n",
    "      passed=True\n",
    "      continue\n",
    "    else:\n",
    "      candidates = []\n",
    "      for single_prompt in a_prompt_set:\n",
    "        passed = False\n",
    "        passat1, bleu =  evaluate_prompt(test_cases=final_test_cases[idx], prompt=single_prompt, solution=human_eval['test'][idx]['canonical_solution'], model_to_test = model_to_test,prompt_index=idx)\n",
    "        # print(single_prompt)\n",
    "        # print(passat1)\n",
    "        # print(f'bleu is {bleu}')\n",
    "        candidates.append([single_prompt, passat1, bleu])\n",
    "        if passat1 == 1:\n",
    "          base_prompts_re[idx] = [single_prompt]\n",
    "          print(f'PAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASED for idx {idx}')\n",
    "          passed = True\n",
    "          break\n",
    "      if passed:\n",
    "        continue\n",
    "      print('here')\n",
    "      ## normalized score\n",
    "      sum_bleu = sum(row[2] for row in candidates)\n",
    "      sum_pass = sum(row[1] for row in candidates)\n",
    "      sum_bleu = 1 if sum_bleu == 0 else sum_bleu\n",
    "      sum_pass = 1 if sum_pass == 0 else sum_pass\n",
    "      for idx2,candid in enumerate(candidates):\n",
    "        candidates[idx2][1] = candidates[idx2][1]/sum_pass\n",
    "        candidates[idx2][2] = candidates[idx2][2]/sum_bleu\n",
    "      ##\n",
    "      next_generation_prompts = []\n",
    "      number_of_generations_by_mutations = 2\n",
    "      number_of_generations_by_crossover = 2\n",
    "      straight_of_generations_by_mutations = 1\n",
    "      ## mutation\n",
    "      selected_candidates_for_mutations = choose_candidates(candidates,number_of_generations_by_mutations)\n",
    "      for a_candidate in selected_candidates_for_mutations:\n",
    "        llama_prompts_final = mutate_prompts_api(a_candidate)\n",
    "        time.sleep(3)\n",
    "        # print('-------------------------------')\n",
    "        # print(llama_prompts_final)\n",
    "        # print('-------------------------------')\n",
    "        next_generation_prompts.append(llama_prompts_final)\n",
    "      ##crossover\n",
    "      for j in range(number_of_generations_by_crossover):\n",
    "          llama_prompts_final = crossover_prompts_api(candidates)\n",
    "          time.sleep(3)\n",
    "          next_generation_prompts.append(llama_prompts_final)\n",
    "      ## straight select\n",
    "      next_generation_prompts.extend(choose_candidates(candidates, straight_of_generations_by_mutations))\n",
    "      # print(f'nexxxxxxxxxxxxxxxxxxxxxxxxxx for {idx}')\n",
    "      # print(next_generation_prompts)\n",
    "      base_prompts_re[idx] = next_generation_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878e6ee7-8979-43ab-93d0-ab969161437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ebd003-9f04-49a1-aa7b-603f8485518a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366ab4a-0042-4251-ae22-e903d72943f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
