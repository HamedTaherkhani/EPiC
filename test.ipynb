{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67bb329-d07f-48ba-9108-287ff47126b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6087623/hamedth/venv3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n",
    "import torch\n",
    "codeLLama_tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-python-hf\", device_map='cuda:0')\n",
    "codeLLama_model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-python-hf\", device_map='auto')\n",
    "# codeLLama_model = codeLLama_model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03332c8e-c6e7-4f91-ac84-8df6db1f16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "def run_generation(dataloader, tokenizer, model, accelerator):\n",
    "    model, dataloader = accelerator.prepare(model, dataloader)\n",
    "\n",
    "    output_sequences = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "       with torch.inference_mode():\n",
    "            generated_tokens = model.generate(**batch)\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "            generated_tokens = accelerator.gather_for_metrics(generated_tokens).cpu().tolist()\n",
    "\n",
    "        outputs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_tokens]\n",
    "        output_sequences.extend(outputs)\n",
    "\n",
    "    return output_sequences\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "  codeLLama_tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-python-hf\")\n",
    "  codeLLama_model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-python-hf\")\n",
    "  dataloader = get_dataloader(...)\n",
    "\n",
    "model, dataloader = accelerator.prepare(model, dataloader)\n",
    "output_sequences = run_generation(dataloader, tokenizer, model, accelerator)\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "  save_output(output_sequences, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd540704-231e-4488-ba27-7122b4d411c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import PartialState  # Can also be Accelerator or AcceleratorState\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"codellama/CodeLlama-7b-instruct-hf\", torch_dtype=torch.float16)\n",
    "distributed_state = PartialState()\n",
    "pipe.to(distributed_state.device)\n",
    "\n",
    "# Assume two processes\n",
    "with distributed_state.split_between_processes([\"a dog\", \"a cat\"]) as prompt:\n",
    "    result = pipe(prompt)\n",
    "    result.save(f\"result_{distributed_state.process_index}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504e1b6-e6e0-470c-ae43-bd23bc1bddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6087623/hamedth/venv3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config('num_processes: 2')  # Write a config file\n",
    "os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506b68dc-96b7-4eaf-8152-b37f32dcb672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6087623/hamedth/venv3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf7e2da-5ba5-4d3f-982c-b8bfc20cc081",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6087623/hamedth/venv3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello this is GPU 0']\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# each GPU creates a string\n",
    "message=[ f\"Hello this is GPU {accelerator.process_index}\" ] \n",
    "\n",
    "# collect the messages from all GPUs\n",
    "messages=gather_object(message)\n",
    "\n",
    "# output the messages only on the main process with accelerator.print() \n",
    "accelerator.print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c396f-7a4c-4c5c-8e59-da7343c41b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from statistics import mean\n",
    "import torch, time, json\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# 10*10 Prompts. Source: https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books\n",
    "prompts_all=[\n",
    "    \"The King is dead. Long live the Queen.\",\n",
    "    \"Once there were four children whose names were Peter, Susan, Edmund, and Lucy.\",\n",
    "    \"The story so far: in the beginning, the universe was created.\",\n",
    "    \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",\n",
    "    \"The sweat wis lashing oafay Sick Boy; he wis trembling.\",\n",
    "    \"124 was spiteful. Full of Baby's venom.\",\n",
    "    \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\",\n",
    "    \"I write this sitting in the kitchen sink.\",\n",
    "    \"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold.\",\n",
    "] * 10\n",
    "\n",
    "# load a base model and tokenizer\n",
    "model_path=\"models/llama2-7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,    \n",
    "    device_map={\"\": accelerator.process_index},\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)   \n",
    "\n",
    "# sync GPUs and start the timer\n",
    "accelerator.wait_for_everyone()\n",
    "start=time.time()\n",
    "\n",
    "# divide the prompt list onto the available GPUs \n",
    "with accelerator.split_between_processes(prompts_all) as prompts:\n",
    "    # store output of generations in dict\n",
    "    results=dict(outputs=[], num_tokens=0)\n",
    "\n",
    "    # have each GPU do inference, prompt by prompt\n",
    "    for prompt in prompts:\n",
    "        prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=100)[0]\n",
    "\n",
    "        # remove prompt from output \n",
    "        output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "        # store outputs and number of tokens in result{}\n",
    "        results[\"outputs\"].append( tokenizer.decode(output_tokenized) )\n",
    "        results[\"num_tokens\"] += len(output_tokenized)\n",
    "\n",
    "    results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "# collect results from all the GPUs\n",
    "results_gathered=gather_object(results)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    timediff=time.time()-start\n",
    "    num_tokens=sum([r[\"num_tokens\"] for r in results_gathered ])\n",
    "\n",
    "    print(f\"tokens/sec: {num_tokens//timediff}, time {timediff}, total tokens {num_tokens}, total prompts {len(prompts_all)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
